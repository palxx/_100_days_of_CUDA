# -*- coding: utf-8 -*-
"""cuda_day_09.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Mk2sLPvKZ8poRakVIol8Wl0OJVAjv_d
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile cuda_day_09.cu
# #include <iostream>
# #include <cuda_runtime.h>
# #include <math_constants.h>
# #include <cmath>
# 
# #define SRAM_SIZE 1024                // M: SRAM size
# #define sequence_length 2             // N: sequence length
# #define embed_dimension 2             // d: embedding dimension
# 
# // Define block sizes
# constexpr int Block_column_size = SRAM_SIZE / (4 * embed_dimension);  // Bc
# constexpr int Block_row_size = std::min(SRAM_SIZE / (4 * embed_dimension), embed_dimension);  // Br
# 
# // Ensure no division by zero
# static_assert(Block_column_size > 0, "Block_column_size must be greater than 0");
# static_assert(Block_row_size > 0, "Block_row_size must be greater than 0");
# 
# // Compute total row and column blocks
# constexpr int Total_row_blocks = (sequence_length + Block_row_size - 1) / Block_row_size;  // Tr
# constexpr int Total_column_blocks = (sequence_length + Block_column_size - 1) / Block_column_size;  // Tc
# 
# __global__ void flashAttentionForward(
#     const float *Query, const float *Key, const float *Value,
#     float *Output, float *max_values, float *sum_values, const float attention_scale)
# {
#     int thread_idx = threadIdx.x;
#     int block_idx = blockIdx.x;
# 
#     // Shared memory blocks
#     float Query_block[Block_row_size * embed_dimension];
#     float Key_block[Block_column_size * embed_dimension];
#     float Value_block[Block_column_size * embed_dimension];
# 
#     for (int col_block = 0; col_block < Total_column_blocks; ++col_block)
#     {
#         // ðŸ”¹ Print which thread is loading Key and Value blocks
#         if (thread_idx < Block_column_size) {
#             for (int d = 0; d < embed_dimension; ++d) {
#                 Key_block[thread_idx * embed_dimension + d] =
#                     Key[col_block * Block_column_size * embed_dimension + thread_idx * embed_dimension + d];
# 
#                 Value_block[thread_idx * embed_dimension + d] =
#                     Value[col_block * Block_column_size * embed_dimension + thread_idx * embed_dimension + d];
# 
#                 printf("[Block %d, Thread %d] Loaded Key[%d] = %.2f, Value[%d] = %.2f\n",
#                         block_idx, thread_idx,
#                         thread_idx * embed_dimension + d, Key_block[thread_idx * embed_dimension + d],
#                         thread_idx * embed_dimension + d, Value_block[thread_idx * embed_dimension + d]);
#             }
#         }
#         __syncthreads();
# 
#         for (int row_block = 0; row_block < Total_row_blocks; ++row_block)
#         {
#             if (thread_idx < Block_row_size) {
#                 // Load Query block
#                 for (int d = 0; d < embed_dimension; ++d) {
#                     Query_block[thread_idx * embed_dimension + d] =
#                         Query[row_block * Block_row_size * embed_dimension + thread_idx * embed_dimension + d];
#                 }
# 
#                 // ðŸ”¹ Print which thread is loading Query block
#                 printf("[Block %d, Thread %d] Loaded Query[%d] = %.2f\n",
#                        block_idx, thread_idx,
#                        thread_idx * embed_dimension, Query_block[thread_idx * embed_dimension]);
#             }
#             __syncthreads();
# 
#             // Compute attention scores for this row
#             if (thread_idx < Block_row_size) {
#                 float row_max = -1e20;
#                 for (int k = 0; k < Block_column_size; ++k) {
#                     float score = 0.0f;
#                     for (int d = 0; d < embed_dimension; ++d) {
#                         score += Query_block[thread_idx * embed_dimension + d] *
#                                  Key_block[k * embed_dimension + d];
#                     }
#                     score *= attention_scale;
#                     row_max = fmaxf(row_max, score);
# 
#                     // ðŸ”¹ Print computed attention scores
#                     printf("[Block %d, Thread %d] Computed attention score: %.2f\n", block_idx, thread_idx, score);
#                 }
#             }
#             __syncthreads();
#         }
#     }
# }
# 
# int main()
# {
#     // Allocate host memory
#     float (*Query)[embed_dimension] = new float[sequence_length][embed_dimension];
#     float (*Key)[embed_dimension] = new float[sequence_length][embed_dimension];
#     float (*Value)[embed_dimension] = new float[sequence_length][embed_dimension];
#     float (*Output)[embed_dimension] = new float[sequence_length][embed_dimension];
#     float *sum_values = new float[sequence_length]();
#     float *max_values = new float[sequence_length];
# 
#     // Initialize values
#     for (int i = 0; i < sequence_length; i++) {
#         for (int j = 0; j < embed_dimension; j++) {
#             Query[i][j] = 2.0f * rand() / RAND_MAX - 1.0f;
#             Key[i][j] = 2.0f * rand() / RAND_MAX - 1.0f;
#             Value[i][j] = 2.0f * rand() / RAND_MAX - 1.0f;
#             Output[i][j] = 0.0f;
#         }
#     }
# 
#     // Allocate device memory
#     float *device_Query, *device_Key, *device_Value, *device_Output;
#     float *device_max_values, *device_sum_values;
# 
#     cudaMalloc(&device_Query, sequence_length * embed_dimension * sizeof(float));
#     cudaMalloc(&device_Key, sequence_length * embed_dimension * sizeof(float));
#     cudaMalloc(&device_Value, sequence_length * embed_dimension * sizeof(float));
#     cudaMalloc(&device_Output, sequence_length * embed_dimension * sizeof(float));
#     cudaMalloc(&device_sum_values, sequence_length * sizeof(float));
#     cudaMalloc(&device_max_values, sequence_length * sizeof(float));
# 
#     // Copy data from host to device
#     cudaMemcpy(device_Query, Query, sequence_length * embed_dimension * sizeof(float), cudaMemcpyHostToDevice);
#     cudaMemcpy(device_Key, Key, sequence_length * embed_dimension * sizeof(float), cudaMemcpyHostToDevice);
#     cudaMemcpy(device_Value, Value, sequence_length * embed_dimension * sizeof(float), cudaMemcpyHostToDevice);
#     cudaMemcpy(device_Output, Output, sequence_length * embed_dimension * sizeof(float), cudaMemcpyHostToDevice);
#     cudaMemcpy(device_sum_values, sum_values, sequence_length * sizeof(float), cudaMemcpyHostToDevice);
#     cudaMemcpy(device_max_values, max_values, sequence_length * sizeof(float), cudaMemcpyHostToDevice);
# 
#     // Compute attention scaling factor
#     float attention_scale = 1.0f / sqrt(embed_dimension);
# 
#     // Launch configuration
#     dim3 block_dim(Block_row_size);
#     dim3 grid_dim(1);
# 
#     // Launch kernel
#     flashAttentionForward<<<grid_dim, block_dim>>>(
#         device_Query, device_Key, device_Value,
#         device_Output, device_max_values, device_sum_values, attention_scale);
# 
#     // Check for errors
#     cudaError_t cudaStatus = cudaGetLastError();
#     if (cudaStatus != cudaSuccess) {
#         fprintf(stderr, "Kernel launch failed: %s\n", cudaGetErrorString(cudaStatus));
#         goto Error;
#     }
# 
#     // Copy results back to host
#     cudaMemcpy(Output, device_Output, sequence_length * embed_dimension * sizeof(float), cudaMemcpyDeviceToHost);
#     cudaMemcpy(max_values, device_max_values, sequence_length * sizeof(float), cudaMemcpyDeviceToHost);
#     cudaMemcpy(sum_values, device_sum_values, sequence_length * sizeof(float), cudaMemcpyDeviceToHost);
# 
#     // Print Query
#     std::cout << "Query:\n";
#     for (int i = 0; i < sequence_length; i++) {
#         for (int j = 0; j < embed_dimension; j++) {
#             std::cout << Query[i][j] << " ";
#         }
#         std::cout << std::endl;
#     }
# 
# Error:
#     cudaFree(device_Query);
#     cudaFree(device_Key);
#     cudaFree(device_Value);
#     cudaFree(device_Output);
#     cudaFree(device_max_values);
#     cudaFree(device_sum_values);
# 
#     delete[] Query;
#     delete[] Key;
#     delete[] Value;
#     delete[] Output;
#     delete[] sum_values;
#     delete[] max_values;
# 
#     return cudaStatus == cudaSuccess ? 0 : 1;
# }

!nvcc -arch=sm_75 cuda_day_09.cu -o cuda_day_09

!./cuda_day_09

!nvidia-smi

