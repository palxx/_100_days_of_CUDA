# -*- coding: utf-8 -*-
"""cuda_day_07.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRjB_ZM5EJhhpBtgsQ3GWEjvE0iU0-di
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile cuda_day_07.cu
# #include <iostream>
# #include <cuda_runtime.h>
# 
#   __global__ void conv1D(float *X, float *Y, float *K, int input_size, int kernel_size){
# 
#   extern __shared__ float shared[];
# 
#   int i = blockIdx.x * blockDim.x + threadIdx.x;
#   printf("blockDim.x %d", blockDim.x);
#   printf("Thread %d (Block %d): Computing Y[%d]\n", threadIdx.x, blockIdx.x, i);
# 
#   int radius = kernel_size / 2;
# 
#   int sharedIdx = threadIdx.x + radius;
#   printf("sharedIdx = %d\n", sharedIdx);
# 
#   if(threadIdx.x < blockDim.x - radius)
#   {
#     int left = i - radius;
#     int right = i + blockDim.x;
# 
#     if (threadIdx.x < radius) {
#     shared[threadIdx.x] = (i - radius >= 0) ? X[i - radius] : 0.0f;
#     }
#     shared[sharedIdx] = X[i]; // ✅ Load center element
#     if (threadIdx.x >= blockDim.x - radius) {
#         shared[sharedIdx + radius] = (i + radius < input_size) ? X[i + radius] : 0.0f;
# }
#   }
# 
#   __syncthreads();
# 
#   float sum = 0.0;
#   for(int j = -radius; j <= radius; j++){
#     sum += shared[sharedIdx + j] * K[radius + j];
#   }
# 
#   if (i < input_size) {
#     Y[i] = sum;
#   }
# 
# }
# 
# int main(){
#   int N = 1024;
#   int BlockSize = 256;
#   int GridSize = (N + BlockSize - 1) / BlockSize;
#   std::cout << "GridSize: " << GridSize << std::endl;
# 
#   int kernelsize = 5;
#   float kernel[kernelsize] = {1.0f, 2.0f, 1.0, 1.0f, -2.0f};
# 
#   int radius = kernelsize / 2;
#   int sharedMemory = (BlockSize + 2 * radius) * sizeof(float);
#   std::cout << "sharedMemory: " << sharedMemory << std::endl;
# 
#   float *Xcpu, *Ycpu;
#   float *Xgpu, *Ygpu, *Kgpu;
# 
#   Xcpu = (float *)malloc(N * sizeof(float));
#   Ycpu = (float *)malloc(N * sizeof(float));
# 
#   for(int i = 0; i < N; i++){
#     Xcpu[i] = 1;
#   }
# 
#   cudaMalloc((void **)&Xgpu, N * sizeof(float));
#   cudaMalloc((void **)&Ygpu, N * sizeof(float));
#   cudaMalloc((void **)&Kgpu, kernelsize * sizeof(float));
#   cudaMemcpy(Xgpu, Xcpu, N * sizeof(float), cudaMemcpyHostToDevice);
#   cudaMemcpy(Kgpu, kernel, kernelsize * sizeof(float), cudaMemcpyHostToDevice);
# 
#   conv1D<<<GridSize, BlockSize, sharedMemory>>>(Xgpu, Ygpu, Kgpu, N, kernelsize);
#   cudaDeviceSynchronize(); // ✅ Ensure GPU execution finishes
# 
#   cudaMemcpy(Ycpu, Ygpu, N * sizeof(float), cudaMemcpyDeviceToHost);
# 
#   std::cout << "First 10 elements: " << std::endl;
# 
#   for(size_t i = 0; i < 10; i++){
#     std::cout << Xcpu[i] << " ";
#   }
# 
#   std::cout << "\n First 10 elements after applying 1dConv: " << std::endl;
# 
#   for(size_t i = 0; i < 10; i++){
#   std::cout << Ycpu[i] << " ";
#   }
# 
#   free(Xcpu);
#   free(Ycpu);
#   cudaFree(Xgpu);
#   cudaFree(Ygpu);
#   cudaFree(Kgpu);
# 
#   return 0;
# }

!nvcc -arch=sm_75 cuda_day_07.cu -o cuda_day_07

!./cuda_day_07

